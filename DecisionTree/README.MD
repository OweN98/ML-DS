# 决策树 Decision Tree
  决策树是一种基本的分类和回归方法。
  * 决策树由结点(node)和有向边(directed edge)构成。结点分为内部节点(internal node)和叶结点(leaf node)。内部节点表示一个特征/属性，叶节点表示一个类。
  
  * 决策树可以被看作是if-then的集合：
    1. 根节点->叶节点的每一条路构建一条规则
    2. 路径上内部节点的特征对应着规则的条件，叶结点的类对应规则的结论
    3. 互斥且完备：每一个实例都被且只被一条路径或一条规则覆盖。

  * 决策树可以表示给定特征下类的条件概率分布。条件概率分布定义在特征空间的一个划分上。将特征空间划分为互不相交的单元/区域，在每个单元定义一个类的概率分布就构成了一个条件概率分布。

  * 决策树学习
    1. 本质上是从训练集中归纳出分类规则，找出一个与训练集矛盾较小的决策树，同时又很好的泛化能力。
    2. 也可以看做由训练集估计条件概率模型，找出由训练集拟合较好，却对未来数据由较好预测的模型。
    3. 用损失函数表示目标。决策树学习的策略是以损失函数为目标函数的最小化。

  * 决策树生成过程：
    1. 构建根节点，放入所有数据。选择一个最优特征，按照该特征将训练集分割成子集，使得各个子集有一个当前条件下最好的分类。
    2. 若已经有了基本的正确分类，构建叶结点，将子集分到对应叶结点。
    3. 若还有不能被基本正确分类的，对这些子集选择新的最优特征，继续分割，构建结点。
    4. 如此递归。直至所有训练集子集被基本正确分类。当每个子集都被分到叶结点上，便生成了决策树。
    5. 有可能发生过拟合现象。所以要进行**剪枝**，使拥有更好的泛化能力。

## 特征选择
* 选取对训练集有分类能力的特征
* 选择准则：**信息增益(information gain)** 或 **信息增益比(information gain ratio)**

## 信息增益
* 熵(entropy)表示随机变量不确定性的度量。条件熵(conditional entropy)是在已知随机变量X的条件下随机变量Y的不确定性。当熵和条件熵的概率由数据估计得到时，则成为**经验熵(empirical entropy)**和**经验条件熵(emperial conditional entropy)**。
* 信息增益定义为集合的经验熵和某特征给定条件下集合的经验条件熵之差。这个差也成为了**互信息**(mutual information)，和决策树学习中的信息增益等价。

## 信息增益比
* 以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。信息增益比可以校正这一问题。
  $$g_R(D,A)=\frac{g(D,A)}{H_A(D)}$$