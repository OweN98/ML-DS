# KNN k-nearest neighbor
  KNN是一种基本分类与回归方法：给定一个数据训练集，对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该输入实例分给这个类。KNN使用的模型实际上对应于特征空间的划分。  
## 模型由三个基本要素：距离度量、k值、分类决策规则决定。    
  特征空间中，每个训练点xi，距离该点比其他点更近的所有点组成一个区域，叫做区域（cell）。xi的类yi作为单元中所有点的类标记(class label)。    
* 距离度量:  
  特征空间中两个实例点的距离是两个实例点相似程度的反应。KNN一般使用欧式距离(Euclidean distance)或者是Lp距离(Lp distance)或是曼哈顿距离(Manhattan distance)。不同距离度量所确定的最近邻点是不同的。  
* k值的选择： 
  较小的k：使用较小的邻域的训练实例进行预测，学习的近似误差小，但是估计误差大，预测结果对紧邻的实例点非常敏感。如果近邻的实例点是噪声，预测则会出错。k的减小会使整体模型更复杂，容易过拟合。  
  较大的k：可以减小估计误差，但是近似误差会增大，容易发生预测的错误，k增大使模型简单化。  
* 分类决策规则：  
  多数表决规则（majority voting rule）：由输入实例的k个邻近的训练实例中的多数类决定输入实例的类。等价于经验风险最小化。  

## kd树——KNN的实现  
  KNN实现时，主要考虑的问题是如何对训练数据进行快速k近邻搜索。当特征空间维数大，训练数据量大时尤其必要。KNN最简单的实现方法是线性扫描（linear scan），计算实例和每一个训练实例的距离，但是计算量大。  
  为了提高效率，减少计算距离的次数，使用kd tree的方法。  
  基本思想：  
  1. 给定目标点，搜索近邻，首先找到包含目标点的叶节点；  
  2. 从该节点出发，依次回退到父节点；
  3. 不断查找与目标点最近邻的节点，在确定不可能存在更接近的结点时终止。
  * Example：
  <div align=center><img src="kd tree-1.png"></div>